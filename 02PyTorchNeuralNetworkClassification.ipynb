{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Binary classification ->\tTarget can be one of two options, e.g. yes or no \tPredict whether or not someone has heart disease based on their health parameters.\n",
        "2. Multi-class classification ->\tTarget can be one of more than two options \tDecide whether a photo is of food, a person or a dog.\n",
        "3. Multi-label classification ->\tTarget can be assigned more than one option \tPredict what categories should be assigned to a Wikipedia article (e.g. mathematics, science & philosophy)."
      ],
      "metadata": {
        "id": "coKInKUKiv-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Architecture of a classification neural network ->\tNeural networks can come in almost any shape or size, but they typically follow a similar floor plan.\n",
        "1. Getting binary classification data ready ->\tData can be almost anything but to get started we're going to create a simple binary classification dataset.\n",
        "2. Building a PyTorch classification model ->\tHere we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop specific to classification.\n",
        "3. Fitting the model to data (training) ->\tWe've got data and a model, now let's let the model (try to) find patterns in the (training) data.\n",
        "4. Making predictions and evaluating a model (inference) ->\tOur model's found patterns in the data, let's compare its findings to the actual (testing) data.\n",
        "5. Improving a model (from a model perspective) ->\tWe've trained and evaluated a model but it's not working, let's try a few things to improve it.\n",
        "6. Non-linearity ->\tSo far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines?\n",
        "7. Replicating non-linear functions ->\tWe used non-linear functions to help model non-linear data, but what do these look like?\n",
        "8. Putting it all together with multi-class classification ->\tLet's put everything we've done so far for binary classification together with a multi-class classification problem."
      ],
      "metadata": {
        "id": "0Pmrj7OPi9x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture of a classification neural network"
      ],
      "metadata": {
        "id": "mK5Yn5dhjRKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get into writing code, let's look at the general architecture of a classification neural network.\n",
        "Hyperparameter \tBinary Classification \tMulticlass classification\n",
        "1. Input layer shape (in_features) ->\tSame as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) \tSame as binary classification\n",
        "2. Hidden layer(s) ->\tProblem specific, minimum = 1, maximum = unlimited \tSame as binary classification\n",
        "3. Neurons per hidden layer ->\tProblem specific, generally 10 to 512 \tSame as binary classification\n",
        "4. Output layer shape (out_features) ->\t1 (one class or the other) \t1 per class (e.g. 3 for food, person or dog photo)\n",
        "5. Hidden layer activation ->\tUsually ReLU (rectified linear unit) but can be many others \tSame as binary classification\n",
        "6. Output activation ->\tSigmoid (torch.sigmoid in PyTorch) \tSoftmax (torch.softmax in PyTorch)\n",
        "7. Loss function ->\tBinary crossentropy (torch.nn.BCELoss in PyTorch) \tCross entropy (torch.nn.CrossEntropyLoss in PyTorch)\n",
        "8. Optimizer ->\tSGD (stochastic gradient descent), Adam (see torch.optim for more options) \tSame as binary classification"
      ],
      "metadata": {
        "id": "L-WAGmrKjany"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make classification data and get it ready"
      ],
      "metadata": {
        "id": "jr8TAJuRjyVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3PjrLSnqikwb"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "\n",
        "# Make 1000 samples\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03, # a little bit of noise to the dots\n",
        "                    random_state=42) # keep random state so we get the same values"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jK28xvhj4i8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}